import math

import torch

from soft_prompting import SoftPrompt, soft_prompts
from soft_prompting.resnet_soft_prompt import ResidualLayer


class SharedResnetSoftPrompt(SoftPrompt):
    """
    A soft prompt which is generated by a single resnet-style model shared by all soft prompt tokens.
    Positional information is provided to the shared model to vary soft prompt embeddings.
    """

    def __init__(self, soft_prompt_token_count: int, input_size: int, output_embedding_size: int,
                 residual_width: int, hidden_layer_count: int, hidden_layer_width: int, device: torch.device = None):
        """
        Creates a shared resnet soft prompt.
        :param soft_prompt_token_count: Number of tokens in the soft prompt.
        :param output_embedding_size: Size of the embedding to create.
        :param residual_width: Width of the residual layers in the soft prompt generator.
        :param hidden_layer_count: Number of residual layers in the soft prompt generator.
        :param hidden_layer_width: Width of the hidden layers in the soft prompt generator.
        :param device: Device to create the soft prompt on.
        """
        super().__init__(soft_prompt_token_count)
        bit_count = int(math.ceil(math.log2(soft_prompt_token_count))) if soft_prompt_token_count > 0 else 0
        pow2_queries = 2 ** torch.arange(bit_count, device=device, dtype=torch.int)
        indices = torch.arange(self.soft_prompt_token_count, device=device, dtype=torch.int)
        position_encoding = (indices.unsqueeze(1).expand(-1, bit_count) &
                             pow2_queries.unsqueeze(0).expand(self.soft_prompt_token_count, -1)).ne(0).float()
        # Ensure the position encoding gets carried along for the ride if the module swaps devices.
        self.register_buffer('position_encoding', position_encoding)
        self.residual_width = residual_width
        self.output_embedding_size = output_embedding_size
        self.hidden_layer_width = hidden_layer_width
        # Note inclusion of the positional encoding with the input size.
        self.to_residual = torch.nn.Linear(input_size + bit_count, residual_width, device=device)
        self.to_residual_nonlinearity = torch.nn.Mish()
        # The parameters for each token are generated by a separate invocation of the same resnet model.
        self.residual_layers = torch.nn.ModuleList(
            [ResidualLayer(residual_width, hidden_layer_width, device=device) for _ in range(hidden_layer_count)])
        self.to_embedding = torch.nn.Linear(residual_width, output_embedding_size, device=device)

    def forward(self, soft_prompt_parameters: torch.Tensor):
        # Shared resnet soft prompts are generated by a resnet-style model, but the parameters
        # of the model are shared across all soft prompt tokens.
        # Positional information is given as a simple bitwise encoding of the token index.
        # The model takes a tensor of shape [batch_count, input_size] and returns a tensor of shape
        # [batch_count, soft_prompt_token_count, embedding_size].

        if self.soft_prompt_token_count == 0:
            return torch.empty([soft_prompt_parameters.size(0), 0, self.output_embedding_size], dtype=torch.float32,
                               device=soft_prompt_parameters.device)
        else:
            parameters_with_position = torch.cat(
                [soft_prompt_parameters.unsqueeze(1).expand(-1, self.soft_prompt_token_count, -1),
                 self.position_encoding.expand(soft_prompt_parameters.size(0), -1, -1)], dim=2)
            token_residuals = self.to_residual_nonlinearity(self.to_residual(parameters_with_position))
            for layer in self.residual_layers:
                token_residuals = layer(token_residuals)
            token_embeddings = self.to_embedding(token_residuals)
            return token_embeddings

    @property
    def input_parameter_count(self) -> int:
        return self.to_residual.in_features

    def get_metadata(self) -> dict:
        return {"soft_prompt_token_count": self.soft_prompt_token_count,
                "input_size": self.input_parameter_count, "output_embedding_size": self.output_embedding_size,
                "residual_width": self.residual_width, "hidden_layer_count": len(self.residual_layers),
                "hidden_layer_width": self.hidden_layer_width}


class SharedResnetFactory:
    def __init__(self, residual_width, hidden_layer_count, hidden_width):
        self.residual_width = residual_width
        self.hidden_layer_count = hidden_layer_count
        self.hidden_width = hidden_width

    def __call__(self, soft_prompt_token_count: int, input_size: int, embedding_size: int) -> soft_prompts.SoftPrompt:
        return SharedResnetSoftPrompt(soft_prompt_token_count, input_size, embedding_size,
                                      self.residual_width, self.hidden_layer_count, self.hidden_width)
