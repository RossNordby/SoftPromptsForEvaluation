import torch

from soft_prompting import soft_prompts
from soft_prompting.soft_prompts import SoftPrompt


class ResidualLayer(torch.nn.Module):
    """
    A single residual layer in a resnet-style model.
    Includes a layer normalization step.
    """

    def __init__(self, residual_width: int, hidden_width: int, device: torch.device = None):
        super().__init__()
        self.residual_to_hidden = torch.nn.Linear(residual_width, hidden_width, device=device)
        self.nonlinearity = torch.nn.Mish()
        self.hidden_to_residual = torch.nn.Linear(hidden_width, residual_width, device=device)
        self.layer_norm = torch.nn.LayerNorm(residual_width, device=device)

    def forward(self, residual: torch.Tensor):
        return self.layer_norm(self.hidden_to_residual(self.nonlinearity(self.residual_to_hidden(residual))) + residual)


class ResnetSoftPrompt(SoftPrompt):
    """
    A soft prompt which is generated by a resnet-style model.
    """

    def __init__(self, soft_prompt_token_count: int, input_size: int, output_embedding_size: int,
                 residual_width: int, hidden_layer_count: int, hidden_layer_width: int, device: torch.device = None):
        """
        Creates a resnet soft prompt.
        :param soft_prompt_token_count: Number of tokens in the soft prompt.
        :param output_embedding_size: Size of the embedding to create.
        :param residual_width: Width of the residual layers in the soft prompt generator.
        :param hidden_layer_count: Number of residual layers in the soft prompt generator.
        :param hidden_layer_width: Width of the hidden layers in the soft prompt generator.
        :param device: Device to create the soft prompt on.
        """
        super().__init__(soft_prompt_token_count)
        self.residual_width = residual_width
        self.output_embedding_size = output_embedding_size
        self.hidden_layer_width = hidden_layer_width
        self.to_residual = torch.nn.Linear(input_size, soft_prompt_token_count * residual_width, device=device)
        self.to_residual_nonlinearity = torch.nn.Mish()
        # The parameters for each token are generated by a separate resnet model
        # Organizing as [layer_count, token_count] instead of [token_count, layer_count] on the assumption
        # that pytorch would sequentialize the latter. Worth testing.
        self.residual_layers = torch.nn.ModuleList([torch.nn.ModuleList(
            [ResidualLayer(residual_width, hidden_layer_width, device=device) for _ in
             range(soft_prompt_token_count)]) for _ in range(hidden_layer_count)])
        self.to_embedding = torch.nn.ModuleList(
            torch.nn.Linear(residual_width, output_embedding_size, device=device) for _ in
            range(soft_prompt_token_count))

    def forward(self, soft_prompt_parameters: torch.Tensor):
        # Residual soft prompts are generated by a resnet-style model.
        # The model takes a tensor of shape [batch_count, input_size] and returns a tensor of shape
        # [batch_count, soft_prompt_token_count, embedding_size].
        if self.soft_prompt_token_count == 0:
            return torch.empty([soft_prompt_parameters.size(0), 0, self.output_embedding_size], dtype=torch.float32,
                               device=soft_prompt_parameters.device)
        else:
            all_token_residuals = self.to_residual_nonlinearity(
                self.to_residual(soft_prompt_parameters).view(-1, self.soft_prompt_token_count, self.residual_width))
            # Treat the execution of each token as independent to avoid pointless per-step stacking and slicing.
            token_residuals = [all_token_residuals[:, i, :] for i in range(self.soft_prompt_token_count)]
            for layer in self.residual_layers:
                for token_index in range(self.soft_prompt_token_count):
                    token_residuals[token_index] = layer[token_index](token_residuals[token_index])
            token_embeddings = [self.to_embedding[token_index](token_residuals[token_index]) for token_index in
                                range(self.soft_prompt_token_count)]
            return torch.stack(token_embeddings, dim=1)

    @property
    def input_parameter_count(self) -> int:
        return self.to_residual.in_features

    def get_metadata(self) -> dict:
        return {"soft_prompt_token_count": self.soft_prompt_token_count,
                "input_size": self.input_parameter_count, "output_embedding_size": self.output_embedding_size,
                "residual_width": self.residual_width, "hidden_layer_count": len(self.residual_layers),
                "hidden_layer_width": self.hidden_layer_width}


class ResnetFactory:
    def __init__(self, residual_width, hidden_layer_count, hidden_width):
        self.residual_width = residual_width
        self.hidden_layer_count = hidden_layer_count
        self.hidden_width = hidden_width

    def __call__(self, soft_prompt_token_count: int, input_size: int, embedding_size: int) -> soft_prompts.SoftPrompt:
        return ResnetSoftPrompt(soft_prompt_token_count, input_size, embedding_size,
                                self.residual_width, self.hidden_layer_count, self.hidden_width)
